{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'maze_l1' from 'e:\\\\OneDrive\\\\Documenten\\\\GitHub\\\\Lab-1-Reinforcement-Learning\\\\maze_l1.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import maze_l1 as mz\n",
    "import numpy as np\n",
    "import importlib\n",
    "importlib.reload(mz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) MDP formulation\n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "Within this model there are two objects that are continuously moving. Namely the player and the minotaur. The player can move in any direction (up, down, left, right) or stay in the same position. The minotaur can move in any direction (up, down, left, right). The state space is the set of all possible positions of the player and the minotaur in the maze. Moreover if the player is caught then it is impossible to get any future rewards, or move out of the current position\n",
    "\n",
    "$$\\mathcal{S} = \\big\\lbrace ((i,j), (k,l)):\\textrm{such that the cell\n",
    "} (i,j) \\textrm{ is not an obstacle or (k, l) and (k,l) is not a boundary} \\big\\rbrace | caught.$$\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We cannot control the minotaur, but we can control the player. Therefore we have five options: move `left`, `right`, `down`, `up` and `stay`. Just like lab0 we allow the player to move in all directions, and move into a \"wall\", but the state will not change and the reward will be a large negative number to prevent this from happening. \n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "\n",
    "- At a state (or position) $s$ taking any action will always lead to a different state (position or caught) $s'$, then $\\mathbb{P}(s' \\vert s, a) = \\frac{1}{m}$. This is due to the minotaur always being able to move and it not going into the walls. Note that it depends on the amount of possible moves for the minotaur, called $m$. If the player is caught then it will always go to the caught state $caught$, then $\\mathbb{P}(s' \\vert caught"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", a) = 1$.\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles and the minotaur.    \n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to the minotaur then $r(s,a) = -\\infty$\n",
    "   - If at state $caught$, taking anny action $a$ lead to $r(caught,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1b) difference when the player and the minotaur move asynchronously\n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "Within this model there are two objects that are continuously moving. namely the player and the minotaur. The player can move in any direction (up, down, left, right) or stay in the same position. The minotaur can move in any direction (up, down, left, right). Now the player and the minotaur move at alternating times. The state space is the set of all possible positions of the player and the minotaur in the maze.  There is also a state where the player is caught.\n",
    "\n",
    "$$\\mathcal{S} = \\big\\lbrace (i,j):\\textrm{such that the cell\n",
    "} (i,j) \\textrm{ is not an obstacle nor the position of the minotaur }\\big\\rbrace | caught.$$\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We cannot control the minotaur, but we can control the player. Therefore we have five options: move `left`, `right`, `down`, `up` and `stay`. Just like lab0 we allow the player to move in all directions, and move into a \"wall\", but the state will not change and the reward will be a large negative number to prevent this from happening. \n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "\n",
    "- At a state (or position) $s$ taking any action will always lead to a different state (or position) $s'$, then $\\mathbb{P}(s' \\vert s, a) = 1$. As the player and the minotaur move alternatingly, we can only control the player movement. Therefore any transition to the next state has a probability of 1.\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles and the minotaur.    \n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to the minotaur then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to a place where the minotaur can move in one timestep then $r(s,a) = -\\infty$\n",
    "   - If the player is in state caught then $r(caught,a), then any action $a$ will lead to $r(caught, a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 0$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"This is the maze, where the following numbers correspond to the following objects:\n",
    "0 = empty cell      -> white\n",
    "1 = obstacle        -> black\n",
    "2 = end             -> light purple\n",
    "3 = start           -> light green\n",
    "4 = minotaur        -> light red\n",
    "\"\"\"\n",
    "import importlib\n",
    "importlib.reload(mz)\n",
    "maze = np.array([\n",
    "    [3, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "\n",
    "env = mz.Maze(maze)\n",
    "\n",
    "# Draw the maze\n",
    "mz.draw_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 20\n",
    "\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shortest path \n",
    "# importlib.reload(mz)\n",
    "\n",
    "# mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate with a static minotaur \n",
    "importlib.reload(mz)\n",
    "\n",
    "method = 'DynProg';\n",
    "starting_position_minotaur = (3, 6)\n",
    "path = env.simulate_solo(policy, method, starting_position_minotaur);\n",
    "\n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate with a moving minotaur (randomly)  \n",
    "importlib.reload(mz)\n",
    "path = env.simulate(policy, method)\n",
    "\n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d) Probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of reaching the exit starting from position A\n",
    "importlib.reload(mz)\n",
    "\n",
    "horizon = 30\n",
    "maze2 = np.array([\n",
    "    [3, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]\n",
    "])\n",
    "\n",
    "env = mz.Maze(maze2, False)\n",
    "\n",
    "# Get the policy\n",
    "V, policy = mz.dynamic_programming(env, horizon)\n",
    "prob_history = env.dynamic_programming_probability_exiting(horizon, policy)\n",
    "\n",
    "# Make a plot from the probability history\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(prob_history)\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('Probability of exiting the maze')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d with movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the policy\n",
    "V, policy = mz.dynamic_programming(env, horizon)\n",
    "\n",
    "env.explain_policy(policy, (6, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mz)\n",
    "\n",
    "env = mz.Maze(maze2, minotaur_stay_enabled = True)\n",
    "\n",
    "# Get the policy\n",
    "V, policy = mz.dynamic_programming(env, horizon)\n",
    "prob_history = env.dynamic_programming_probability_exiting(horizon, policy)\n",
    "\n",
    "# Make a plot from the probability history\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(prob_history)\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('Probability of exiting the maze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuate a shortest path\n",
    "importlib.reload(mz)\n",
    "method = 'DynProg'\n",
    "path = env.simulate(policy, method)\n",
    "\n",
    "mz.animate_solution(maze, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e - Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mz)\n",
    "env = mz.Maze(maze2, False)\n",
    "\n",
    "# Discount Factor\n",
    "gamma   = 0.50\n",
    "\n",
    "# Accuracy treshold\n",
    "epsilon = 0.001\n",
    "\n",
    "# The max number of iterations to run the algorithm\n",
    "max_iter = 30\n",
    "\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon, max_iter)\n",
    "method = 'ValIter'\n",
    "\n",
    "# Simulate with a static minotaur\n",
    "path = env.simulate(policy, method)\n",
    "\n",
    "#Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1f) Simulate 10000 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mz)\n",
    "# Amount of exits\n",
    "exit_n = 0\n",
    "total_n = 0\n",
    "ITERATION_COUNT = 10000\n",
    "\n",
    "# Simulate with a moving minotaur (randomly)\n",
    "for i in range(ITERATION_COUNT):\n",
    "    # Simulate with a static minotaur\n",
    "    path = env.simulate(policy, method)\n",
    "\n",
    "    # If the path ends in the exit, increment the number of exits\n",
    "    if path[-1][0] == (6,5):\n",
    "        exit_n += 1\n",
    "\n",
    "    # Increment the total number of iterations\n",
    "    total_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_n / total_n\n",
    "\n",
    "print(exit_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
