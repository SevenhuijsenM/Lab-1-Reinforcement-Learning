{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maze_l1 as mz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP formulation\n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "Within this model there are two objects that are continuously moving. Namely the player and the minotaur. The player can move in any direction (up, down, left, right) or stay in the same position. The minotaur can move in any direction (up, down, left, right). The state space is the set of all possible positions of the player and the minotaur in the maze. \n",
    "\n",
    "$$\\mathcal{S} = \\big\\lbrace ((i,j), (k,l)):\\textrm{such that the cell\n",
    "} (i,j) \\textrm{ is not an obstacle and (k,l) is not a boundary}\\big\\rbrace.$$\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We cannot control the minotaur, but we can control the player. Therefore we have five options: move `left`, `right`, `down`, `up` and `stay`. Just like lab0 we allow the player to move in all directions, and move into a \"wall\", but the state will not change and the reward will be a large negative number to prevent this from happening. \n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "\n",
    "- At a state (or position) $s$ taking any action will always lead to a different state (or position) $s'$, then $\\mathbb{P}(s' \\vert s, a) = \\frac{1}{m}$. This is due to the minotaur always being able to move and it not going into the walls. Note that it depends on the amount of possible moves for the minotaur, called $m$.\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles and the minotaur.    \n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to the minotaur then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 0$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MAL Sevenhuijsen\\Documents\\School\\Master\\S4\\Reinforcement learning\\Lab1\\lab1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MAL%20Sevenhuijsen/Documents/School/Master/S4/Reinforcement%20learning/Lab1/lab1.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m maze \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MAL%20Sevenhuijsen/Documents/School/Master/S4/Reinforcement%20learning/Lab1/lab1.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MAL%20Sevenhuijsen/Documents/School/Master/S4/Reinforcement%20learning/Lab1/lab1.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MAL%20Sevenhuijsen/Documents/School/Master/S4/Reinforcement%20learning/Lab1/lab1.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MAL%20Sevenhuijsen/Documents/School/Master/S4/Reinforcement%20learning/Lab1/lab1.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MAL%20Sevenhuijsen/Documents/School/Master/S4/Reinforcement%20learning/Lab1/lab1.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m env \u001b[39m=\u001b[39m mz\u001b[39m.\u001b[39;49mMaze(maze)\n",
      "File \u001b[1;32mc:\\Users\\MAL Sevenhuijsen\\Documents\\School\\Master\\S4\\Reinforcement learning\\Lab1\\maze_l1.py:37\u001b[0m, in \u001b[0;36mMaze.__init__\u001b[1;34m(self, maze, weights, random_rewards)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_actions                \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions);\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_states                 \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates);\n\u001b[1;32m---> 37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransition_probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__transitions();\n\u001b[0;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards                  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__rewards(weights\u001b[39m=\u001b[39mweights,\n\u001b[0;32m     39\u001b[0m                                         random_rewards\u001b[39m=\u001b[39mrandom_rewards)\n",
      "File \u001b[1;32mc:\\Users\\MAL Sevenhuijsen\\Documents\\School\\Master\\S4\\Reinforcement learning\\Lab1\\maze_l1.py:112\u001b[0m, in \u001b[0;36mMaze.__transitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_states):\n\u001b[0;32m    111\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_actions):\n\u001b[1;32m--> 112\u001b[0m         next_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__move(s,a);\n\u001b[0;32m    113\u001b[0m         transition_probabilities[next_s, s, a] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m;\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m transition_probabilities\n",
      "File \u001b[1;32mc:\\Users\\MAL Sevenhuijsen\\Documents\\School\\Master\\S4\\Reinforcement learning\\Lab1\\maze_l1.py:74\u001b[0m, in \u001b[0;36mMaze.__move\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Makes a step in the maze, given a current position and an action.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m    If the action STAY or an inadmissible action is used, the agent stays in place.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[39m    :return tuple next_cell: Position (x,y) on the maze that agent transitions to.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m# Compute the future position given current (state, action)\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstates[state][\u001b[39m0\u001b[39;49m] \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[action][\u001b[39m0\u001b[39;49m];\n\u001b[0;32m     75\u001b[0m col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates[state][\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[action][\u001b[39m1\u001b[39m];\n\u001b[0;32m     77\u001b[0m \u001b[39m# Is the future position an impossible one ?\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 2, 0]\n",
    "])\n",
    "\n",
    "env = mz.Maze(maze)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
